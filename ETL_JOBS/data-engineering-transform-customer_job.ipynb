{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"You are already connected to a glueetl session 0921f56a-0d13-4659-a557-9e176e78819d.\n",
						"\n",
						"No change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"The following configurations have been updated: {'--job-bookmark-option': 'job-bookmark-enable'}\n"
					]
				}
			],
			"source": [
				"# enabling the job_bookmark\n",
				"%%configure\n",
				"{\n",
				"    \"--job-bookmark-option\":\"job-bookmark-enable\"\n",
				"}"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n"
					]
				}
			],
			"source": [
				"import sys\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"from pyspark.sql.types import TimestampType\n",
				"from pyspark.sql.functions import current_timestamp, current_date, lit, concat_ws, col, sha2, split\n",
				"from awsglue.dynamicframe import DynamicFrame\n",
				"from awsglue.utils import getResolvedOptions\n",
				"\n",
				"# creating our spark_session\n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"glueContext.spark_session\n",
				"job = Job(glueContext)\n",
				"args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
				"job.init(args['JOB_NAME'],args)\n",
				"\n",
				"# declaring bucket_variables\n",
				"bucket_name = \"data-engineering-project-920372994009\"\n",
				"source_folder_name = \"bronze_data\"\n",
				"processed_folder_name = \"silver_data\"\n",
				"db_name = \"dev\"\n",
				"table_name = \"Customer\"\n",
				"\n",
				"# declaring glue_catalog_variables\n",
				"glue_database = \"data-engineering-project-glue-db\"\n",
				"glue_table_name = \"raw_data_customer\"\n",
				"\n",
				"# creating/reading data from the glue_catalog\n",
				"\n",
				"customer_df_from_catalog = glueContext.create_data_frame_from_catalog(glue_database,glue_table_name,\\\n",
				"                                                                      additional_options = {\"useCatalogSchema\":True,\"useSparkDataSource\":True,\"header\":True},\\\n",
				"                                                                      transformation_ctx = \"customer_df_from_catalog\")\n",
				"\n",
				"if customer_df_from_catalog.count() > 0:\n",
				"    # update the data_type of the columns, rename the columns as needed.\n",
				"    renamed_customer_df = customer_df_from_catalog.withColumnRenamed(\"op\",\"cdc_operations\")\\\n",
				"                                                  .withColumnRenamed(\"customerid\",\"customer_id\")\\\n",
				"                                                  .withColumnRenamed(\"name\",\"customer_name\")\\\n",
				"                                                  .withColumnRenamed(\"email\",\"customer_email\")\\\n",
				"                                                  .withColumnRenamed(\"phone\",\"customer_phone\")\\\n",
				"                                                  .withColumnRenamed(\"address\",\"customer_address\")\\\n",
				"                                                  .withColumnRenamed(\"country\",\"customer_country\")\\\n",
				"                                                  .withColumnRenamed(\"city\",\"customer_city\")\\\n",
				"\n",
				"    # declaring scd_variables\n",
				"    current_date = current_date()\n",
				"    current_timestamp = current_timestamp()\n",
				"    record_end_ts = lit('9999-12-31').cast(TimestampType())\n",
				"    active_flag = lit(1)\n",
				"\n",
				"    hash_value = concat_ws('', \n",
				"                           col(\"customer_name\"), \n",
				"                           col(\"customer_email\"), \n",
				"                           col(\"customer_phone\"), \n",
				"                           col(\"customer_address\"), \n",
				"                           col(\"customer_country\"), \n",
				"                           col(\"customer_city\"))\n",
				"\n",
				"    customer_final_df = renamed_customer_df.withColumn(\"hash_value\", sha2(hash_value, 256))\\\n",
				"                                           .withColumn(\"ingestion_date\",current_date)\\\n",
				"                                           .withColumn(\"record_start_ts\",current_timestamp)\\\n",
				"                                           .withColumn(\"record_end_ts\",record_end_ts)\\\n",
				"                                           .withColumn(\"cust_first_name\", split(renamed_customer_df[\"customer_name\"],\" \")[0])\\\n",
				"                                           .withColumn(\"cust_last_name\", split(renamed_customer_df[\"customer_name\"],\" \")[1])\\\n",
				"                                           .withColumn(\"active_flag\",active_flag)\\\n",
				"                                           .drop(\"customer_name\")\n",
				"\n",
				"    final_dyf_customer = DynamicFrame.fromDF(customer_final_df,glueContext,\"final_dyf_customer\")\n",
				"\n",
				"    glueContext.write_dynamic_frame.from_options(\n",
				"        frame = final_dyf_customer,\n",
				"        connection_type = 's3',\n",
				"        connection_options = {\"path\":f\"s3://{bucket_name}/{processed_folder_name}/{db_name}/{table_name}/\",\"partitionKeys\":[\"ingestion_date\"]},\n",
				"        format = 'parquet',\n",
				"        transformation_ctx = 'customer_dyf_to_s3'\n",
				"    )\n",
				"else:\n",
				"    print(\"No data found in the glue_catalog\")\n",
				"\n",
				"job.commit()"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
