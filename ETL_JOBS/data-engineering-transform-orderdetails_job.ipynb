{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%timeout 20",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session a81013b6-3f1c-4f76-af97-22d35cf07017.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current timeout is 20 minutes.\ntimeout has been set to 20 minutes.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%configure\n{\n    \"--job-bookmark-option\":\"job-bookmark-enable\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session a81013b6-3f1c-4f76-af97-22d35cf07017.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "The following configurations have been updated: {'--job-bookmark-option': 'job-bookmark-enable'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# creating the context and reading data from glue catalog\nimport sys\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql.functions import current_date\nfrom awsglue.utils import getResolvedOptions\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nglueContext.spark_session\njob = Job(glueContext)\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\njob.init(args['JOB_NAME'], args)\n\n# S3 variables\nbucket_name = \"data-engineering-project-920372994009\"\nsource_folder = \"bronze_data\"\nprocessed_folder = \"silver_data\"\ndb_name = \"dev\"\ntable_name = \"OrderDetails\"\n\n# glue variables\nglue_db = \"data-engineering-project-glue-db\"\nglue_table_name = \"raw_data_orderdetails\"\n\norderDetails_df_from_catalog = glueContext.create_data_frame_from_catalog(glue_db, glue_table_name,\n                                                                          additional_options = {\"useCatalogSchema\":True}, \n                                                                          transformation_ctx = \"orderDetails_df_from_catalog\")\n# orderDetails_df_from_catalog.show()\nif orderDetails_df_from_catalog.count() > 0:\n    renamed_orderdetails_df = orderDetails_df_from_catalog.withColumnRenamed(\"orderdetailsid\",\"order_details_id\")\\\n                                                          .withColumnRenamed(\"orderid\", \"order_id\")\\\n                                                          .withColumnRenamed(\"productid\", \"product_id\")\\\n                                                          .withColumnRenamed(\"quantity\", \"product_quantity\")\\\n                                                          .drop(\"op\")\n    \n    # adding columns\n    current_date = current_date()\n    \n    final_orderdetails_df = renamed_orderdetails_df.withColumn(\"ingestion_date\",current_date)\\\n                                                   .withColumn(\"ingestion_date_pk\", current_date)\n    \n    # creating a dynamic_frame and storing the data in S3 as parquet\n    order_details_final_dyf = DynamicFrame.fromDF(final_orderdetails_df, glueContext, \"orderdetails_final_dyf\")\n    \n    glueContext.write_dynamic_frame.from_options(\n        frame = order_details_final_dyf,\n        connection_type = \"s3\",\n        connection_options = {\"path\": f\"s3://{bucket_name}/{processed_folder}/{db_name}/{table_name}/\", \"partitionKeys\":[\"ingestion_date_pk\"]},\n        format = \"parquet\",\n        transformation_ctx = \"orderdetails_dyf_to_s3\"\n    )\nelse:\n    print(\"no data from catalog to be processed\")\n    \njob.commit()",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}