{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%%configure\n{\n    \"--job-bookmark-option\":\"job-bookmark-enable\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "The following configurations have been updated: {'--job-bookmark-option': 'job-bookmark-enable'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# # SPARK\n\n# # defining the resource constants\n# bucket_name = \"data-engineering-project-920372994009\"\n# source_folder = \"bronze_data\"\n# processed_folder = \"silver_data\"\n# db_name = \"dev\"\n# table_name = \"Product\"\n\n# # reading the data using inferring schema\n# df = spark.read.csv(f\"s3://{bucket_name}/{source_folder}/{db_name}/{table_name}\", sep = \",\", header=True, inferSchema=True)\n# df.printSchema()\n# df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.8 \n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n\n# product_schema = StructType(\n#     fields = (\n#         [\n#             StructField(\"op\", StringType()),\n#             StructField(\"productId\", StringType()),\n#             StructField(\"productName\", StringType()),\n#             StructField(\"brandName\", StringType()),\n#             StructField(\"productDescription\", StringType()),\n#             StructField(\"price\", DoubleType()),\n#             StructField(\"productCategory\", StringType()),\n#         ]\n#     )\n# )\n# # reading data from a structType(manually)\n# products_df = spark.read.option(\"header\",\"true\").option(\"delimeter\",\",\").schema(product_schema).csv(f\"s3://{bucket_name}/{source_folder}/{db_name}/{table_name}/\")\n# products_df.printSchema()\n# products_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom pyspark.sql.functions import current_timestamp, current_date, lit, concat_ws, col, sha2, round\nfrom pyspark.sql.types import TimestampType\nfrom awsglue.dynamicframe import DynamicFrame\nfrom awsglue.job import Job\nfrom awsglue.utils import getResolvedOptions\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\nargs = getResolvedOptions(sys.argv,['JOB_NAME'])\njob.init(args['JOB_NAME'], args)\n\n# defining glue resource constants\nglue_database = \"data-engineering-project-glue-db\"\nglue_table = \"raw_data_product\"\n\n# reading data from a glue data catalogue\nproducts_df_from_catalog = glueContext.create_data_frame_from_catalog(glue_database,glue_table,additional_options = {\"useCatalogueSchema\":True}, transformation_ctx = 'products_df_from_catalog')\n\n# renaming the columns for silver_data folder\n# from pyspark.sql.dataframe import withColumnRenamed\nif products_df_from_catalog.count() > 0:\n    renamed_products = products_df_from_catalog.withColumnRenamed(\"op\",\"cdc_operations\")\\\n                              .withColumnRenamed(\"productid\",\"product_id\")\\\n                              .withColumnRenamed(\"productname\",\"product_name\")\\\n                              .withColumnRenamed(\"brandname\",\"brand_name\")\\\n                              .withColumnRenamed(\"productdescription\",\"product_description\")\\\n                              .withColumnRenamed(\"price\",\"product_price\")\\\n                              .withColumnRenamed(\"productcategory\",\"product_category\")\n\n    # apply the slowly changing dimension - 2 here , i.e, using versions/active_flag/date to keep track of changing dimensions\n    current_timestamp = current_timestamp()\n    current_date = current_date()\n    record_end_ts = lit('9999-12-31').cast(TimestampType())\n    active_flag = lit(1)\n\n    # here, we'll be using \"sha2\" function to create a hash_value for each record being updated, \n    # hash_value is basically the encrypted(concatenated values of a row)\n\n    hash_value = concat_ws('',col(\"product_id\"),col(\"product_name\"),col(\"brand_name\"),col(\"product_description\"),col(\"product_price\"),col(\"product_category\"))\n\n    # final_df with scd fields\n\n    final_product_df = renamed_products.withColumn(\"hash_value\", sha2(hash_value, 256))\\\n                                       .withColumn(\"record_start_ts\", current_timestamp)\\\n                                       .withColumn(\"record_end_ts\", record_end_ts)\\\n                                       .withColumn(\"ingestion_date\", current_date)\\\n                                       .withColumn(\"active_flag\", active_flag)\\\n                                       .withColumn(\"product_price\", round(col(\"product_price\"),2))\n\n    # now we'll creating a dynamicframe from this dataframe and store it into processed folder(S3) in parquet format.\n    final_product_dyf = DynamicFrame.fromDF(final_product_df, glueContext, \"final_product_dyf\")\n\n    source_bucket = \"data-engineering-project-920372994009\"\n    processed_folder = \"silver_data\"\n    db_name = \"dev\"\n    table_name = \"Product\"\n\n    glueContext.write_dynamic_frame.from_options(\n        frame = final_product_dyf,\n        connection_type = \"s3\",\n        connection_options = {\"path\": f\"s3://{source_bucket}/{processed_folder}/{db_name}/{table_name}/\", \"partitionKeys\" : [\"ingestion_date\"]},\n        format = 'parquet',\n        transformation_ctx = 'products_dyf_to_s3'\n    )\nelse:\n    print(f\"no data found in {glue_database}.{glue_table}\")\n        \n\njob.commit()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}